{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from six.moves import cPickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading net skeleton with parameters name and shapes.\n",
    "with open(\"util/net_skeleton.ckpt\", \"rb\") as f:\n",
    "    net_skeleton = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_layers    = [2, 2, 3, 3, 3, 1, 1, 1]\n",
    "dilations     = [[1, 1],\n",
    "                 [1, 1],\n",
    "                 [1, 1, 1],\n",
    "                 [1, 1, 1],\n",
    "                 [2, 2, 2],\n",
    "                 [12], \n",
    "                 [1], \n",
    "                 [1]]\n",
    "n_classes = 21\n",
    "ks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_variable(name, shape):\n",
    "    \"\"\"Create a convolution filter variable of the given name and shape,\n",
    "       and initialise it using Xavier initialisation \n",
    "       (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf).\n",
    "    \"\"\"\n",
    "    initialiser = tf.contrib.layers.xavier_initializer_conv2d(dtype=tf.float32)\n",
    "    variable = tf.Variable(initialiser(shape=shape), name=name)\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_bias_variable(name, shape):\n",
    "    \"\"\"Create a bias variable of the given name and shape,\n",
    "       and initialise it to zero.\n",
    "    \"\"\"\n",
    "    initialiser = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    variable = tf.Variable(initialiser(shape=shape), name=name)\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "        \"\"\"\n",
    "        Return a Tensor for a batch of image input\n",
    "        : image_shape: Shape of the images\n",
    "        : return: Tensor for image input.\n",
    "        \"\"\"\n",
    "        # TODO: Implement Function\n",
    "        return tf.placeholder(tf.float32,shape=(None,image_shape[0],image_shape[1],image_shape[2]),name=\"x\")\n",
    "def neural_net_label_input(image_shape):\n",
    "        \"\"\"\n",
    "        Return a Tensor for a batch of image input\n",
    "        : image_shape: Shape of the images\n",
    "        : return: Tensor for image input.\n",
    "        \"\"\"\n",
    "        # TODO: Implement Function\n",
    "        return tf.placeholder(tf.int8,shape=(None,image_shape[0],image_shape[1],image_shape[2]),name=\"y\")\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def _create_variables(weights_path):\n",
    "#         var = list()\n",
    "#         index = 0\n",
    "        \n",
    "#         if weights_path is not None:\n",
    "#             with open(weights_path, \"rb\") as f:\n",
    "#                 weights = cPickle.load(f) # Load pre-trained weights.\n",
    "#                 for name, shape in net_skeleton:\n",
    "#                     var.append(tf.Variable(weights[name],\n",
    "#                                            name=name))\n",
    "#                 del weights\n",
    "#         else:\n",
    "#             # Initialise all weights randomly with the Xavier scheme,\n",
    "#             # and \n",
    "#             # all biases to 0's.\n",
    "#             for name, shape in net_skeleton:\n",
    "#                 if \"/w\" in name: # Weight filter.\n",
    "#                     w = create_variable(name, list(shape))\n",
    "#                     var.append(w)\n",
    "#                 else:\n",
    "#                     b = create_bias_variable(name, list(shape))\n",
    "#                     var.append(b)\n",
    "#         return var\n",
    "# variables = _create_variables(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# current = tf.convert_to_tensor(batch_features_array, dtype=tf.float32)\n",
    "# v_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# w = variables[v_idx * 2]\n",
    "# b = variables[v_idx * 2 + 1]\n",
    "# conv = tf.nn.conv2d(current, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# current = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "# current = tf.nn.max_pool(current, \n",
    "#                                          ksize=[1, ks, ks, 1],\n",
    "#                                          strides=[1, 1, 1, 1],\n",
    "#                                          padding='SAME')\n",
    "# v_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# w = variables[v_idx * 2]\n",
    "# b = variables[v_idx * 2 + 1]\n",
    "# conv = tf.nn.conv2d(current, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# current = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "# v_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tf.pack(current.get_shape()[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#labelBatch = tf.image.resize_nearest_neighbor(tf.convert_to_tensor(batch_labels_array_indexed, dtype=tf.int8), current.get_shape()[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DeepLabLFOVModel(object):\n",
    "    def __init__(self, weights_path=None):\n",
    "        self.variables = self._create_variables(weights_path)\n",
    "    \n",
    "    def _create_variables(self, weights_path):\n",
    "        var = list()\n",
    "        index = 0\n",
    "        \n",
    "        if weights_path is not None:\n",
    "            with open(weights_path, \"rb\") as f:\n",
    "                weights = cPickle.load(f) # Load pre-trained weights.\n",
    "                for name, shape in net_skeleton:\n",
    "                    var.append(tf.Variable(weights[name],\n",
    "                                           name=name))\n",
    "                del weights\n",
    "        else:\n",
    "            # Initialise all weights randomly with the Xavier scheme,\n",
    "            # and \n",
    "            # all biases to 0's.\n",
    "            for name, shape in net_skeleton:\n",
    "                if \"/w\" in name: # Weight filter.\n",
    "                    w = create_variable(name, list(shape))\n",
    "                    var.append(w)\n",
    "                else:\n",
    "                    b = create_bias_variable(name, list(shape))\n",
    "                    var.append(b)\n",
    "        return var\n",
    "    \n",
    "    def _create_network(self, input_batch, keep_prob):\n",
    "        \n",
    "        current = input_batch\n",
    "        v_idx = 0\n",
    "        for b_idx in range(len(dilations) - 1):\n",
    "            for l_idx, dilation in enumerate(dilations[b_idx]):\n",
    "                w = self.variables[v_idx * 2]\n",
    "                b = self.variables[v_idx * 2 + 1]\n",
    "                if dilation == 1:\n",
    "                    conv = tf.nn.conv2d(current, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "                else:\n",
    "                    conv = tf.nn.atrous_conv2d(current, w, dilation, padding='SAME')\n",
    "                current = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "                v_idx += 1\n",
    "                \n",
    "                if b_idx < 3:\n",
    "                    current = tf.nn.max_pool(current, \n",
    "                                         ksize=[1, ks, ks, 1],\n",
    "                                         strides=[1, 2, 2, 1],\n",
    "                                         padding='SAME')\n",
    "                elif b_idx == 3:\n",
    "                    current = tf.nn.max_pool(current, \n",
    "                             ksize=[1, ks, ks, 1],\n",
    "                             strides=[1, 1, 1, 1],\n",
    "                             padding='SAME')\n",
    "                elif b_idx == 4:\n",
    "                    current = tf.nn.max_pool(current, \n",
    "                                         ksize=[1, ks, ks, 1],\n",
    "                                         strides=[1, 1, 1, 1],\n",
    "                                         padding='SAME')\n",
    "                    current = tf.nn.avg_pool(current, \n",
    "                                         ksize=[1, ks, ks, 1],\n",
    "                                         strides=[1, 1, 1, 1],\n",
    "                                         padding='SAME')\n",
    "                elif b_idx <= 6:\n",
    "                    current = tf.nn.dropout(current, keep_prob=keep_prob)\n",
    "        \n",
    "        # Classification layer; no ReLU.\n",
    "        w = self.variables[v_idx * 2]\n",
    "        b = self.variables[v_idx * 2 + 1]\n",
    "        conv = tf.nn.conv2d(current, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        current = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        return current\n",
    "    \n",
    "    \n",
    "    def preds(self, input_batch,keep_prob_input):\n",
    "        \"\"\"Create the network and run inference on the input batch.\n",
    "        \n",
    "        Args:\n",
    "          input_batch: batch of pre-processed images.\n",
    "          \n",
    "        Returns:\n",
    "          Argmax over the predictions of the network of the same shape as the input.\n",
    "        \"\"\"\n",
    "        \n",
    "        raw_output = self._create_network(tf.cast(input_batch, tf.float32), keep_prob_input)\n",
    "        print(raw_output.shape)\n",
    "        raw_output = tf.image.resize_bilinear(raw_output, tf.shape(input_batch)[1:3,])\n",
    "        print(raw_output.shape)\n",
    "        raw_output = tf.argmax(raw_output, dimension=3)\n",
    "        print(raw_output.shape)\n",
    "        raw_output = tf.expand_dims(raw_output, dim=3) # Create 4D-tensor.\n",
    "        print(raw_output.shape)\n",
    "        return tf.cast(raw_output, tf.uint8)\n",
    "    \n",
    "    def loss(self, img_batch, label_batch,keep_prob_input):\n",
    "        \"\"\"Create the network, run inference on the input batch and compute loss.\n",
    "        \n",
    "        Args:\n",
    "          input_batch: batch of pre-processed images.\n",
    "          \n",
    "        Returns:\n",
    "          Pixel-wise softmax loss.\n",
    "        \"\"\"\n",
    "        raw_output = self._create_network(tf.cast(img_batch, tf.float32), keep_prob)\n",
    "        print(raw_output.shape)\n",
    "        prediction = tf.reshape(raw_output, [-1, n_classes])\n",
    "        \n",
    "        labels= tf.image.resize_nearest_neighbor(tf.convert_to_tensor(label_batch, dtype=tf.int8), [4,4])\n",
    "        gt = tf.reshape(labels, [-1, n_classes])\n",
    "        print(label_batch.shape)\n",
    "        # Pixel-wise softmax loss.\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=gt)\n",
    "        reduced_loss = tf.reduce_mean(loss)\n",
    "        return reduced_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_label(label_batch):\n",
    "        colormap = {(0,0,0):0, (128,0,0):1, (0,128,0):2, (128,128,0):3, (0,0,128):4, (128,0,128):5, (0,128,128):6, (128,128,128):7, (64,0,0):8, (192,0,0):9, (64,128,0):10, (192,128,0):11, (64,0,128):12, (192,0,128):13, \n",
    "            (64,128,128):14, (192,128,128):15, (0,64,0):16, (128,64,0):17, (0,192,0):18, (128,192,0):19, (0,64,128):20}                                            \n",
    "        gndTruth = np.zeros((label_batch.shape[0],500,500,21), dtype=np.int)\n",
    "        for i in range(label_batch.shape[0]):\n",
    "            for j in range(500):\n",
    "                for k in range(500):   \n",
    "                    if(colormap.get(tuple(label_batch[i][j,k]))):\n",
    "                        gndTruth[i,j,k,colormap.get(tuple(label_batch[i][j,k]))] = 1\n",
    "                    else:\n",
    "                        gndTruth[i,j,k,0] = 1\n",
    "        return gndTruth\n",
    "def encode_labels(label_batch):\n",
    "        colormap = {(0,0,0):0, (128,0,0):1, (0,128,0):2, (128,128,0):3, (0,0,128):4, (128,0,128):5, (0,128,128):6, (128,128,128):7, (64,0,0):8, (192,0,0):9, (64,128,0):10, (192,128,0):11, (64,0,128):12, (192,0,128):13, \n",
    "            (64,128,128):14, (192,128,128):15, (0,64,0):16, (128,64,0):17, (0,192,0):18, (128,192,0):19, (0,64,128):20}                                            \n",
    "        gndTruth = np.zeros((label_batch.shape[0],500,500), dtype=np.int)\n",
    "        for i in range(label_batch.shape[0]):\n",
    "            for j in range(500):\n",
    "                for k in range(500):   \n",
    "                    if(colormap.get(tuple(label_batch[i][j,k]))):\n",
    "                        gndTruth[i,j,k]=colormap.get(tuple(label_batch[i][j,k]))\n",
    "                    else:\n",
    "                        gndTruth[i,j,k] = 0\n",
    "        return gndTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_preprocess_training_batch(batch_id):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocessed/pre_processed_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = cPickle.load(open(filename, mode='rb'))\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, ?, 21)\n",
      "(?, 500, 500, 21)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-91e568465274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_preprocess_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mbatch_labels_indexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_labels_indexed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeep_probability\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/systems/tensorflow-env3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/systems/tensorflow-env3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/systems/tensorflow-env3/lib/python3.4/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch_size = 256\n",
    "keep_probability = 0.75\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((500, 500, 3))\n",
    "y = neural_net_label_input((500, 500, 21))\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "net = DeepLabLFOVModel()\n",
    "cost = net.loss(x, y,keep_prob)\n",
    "cost_summary=tf.summary.scalar(\"loss\",cost)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "save_model_path = \"./capstone\"\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('/Users/systems/Downloads/udacity/machine-learning-master/projects/capstone/logs/', sess.graph)\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i  in range(0, 2):\n",
    "            batch_features, batch_labels = load_preprocess_training_batch(batch_i)\n",
    "            batch_features_array = np.zeros((len(batch_features),500,500,3))\n",
    "            batch_labels_array = np.zeros((len(batch_features),500,500,3))\n",
    "            i=0\n",
    "            for b in batch_features:\n",
    "                batch_features_array[i] = b\n",
    "                i=i+1\n",
    "            batch_features_array = batch_features_array.astype(np.float32)\n",
    "            i=0\n",
    "            for b in batch_labels:\n",
    "                batch_labels_array[i] = b\n",
    "                i=i+1\n",
    "            batch_labels_array_indexed = prepare_label(batch_labels_array)\n",
    "            _,cost_batch=sess.run([optimizer,cost_summary],feed_dict={x:batch_features_array,y:batch_labels_array_indexed,keep_prob:keep_probability})\n",
    "            file_writer.add_summary(cost_batch, epoch)\n",
    "    saver = tf.train.Saver();  \n",
    "    saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Colour map.\n",
    "label_colours = [(0,0,0)\n",
    "                # 0=background\n",
    "                ,(128,0,0),(0,128,0),(128,128,0),(0,0,128),(128,0,128)\n",
    "                # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "                ,(0,128,128),(128,128,128),(64,0,0),(192,0,0),(64,128,0)\n",
    "                # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "                ,(192,128,0),(64,0,128),(192,0,128),(64,128,128),(192,128,128)\n",
    "                # 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "                ,(0,64,0),(128,64,0),(0,192,0),(128,192,0),(0,64,128)]\n",
    "                # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "    \n",
    "def decode_labels(mask):\n",
    "    \"\"\"Decode batch of segmentation masks.\n",
    "    \n",
    "    Args:\n",
    "      label_batch: result of inference after taking argmax.\n",
    "    \n",
    "    Returns:\n",
    "      An batch of RGB images of the same size\n",
    "    \"\"\"\n",
    "    img = Image.new('RGB', (len(mask[0]), len(mask)))\n",
    "    pixels = img.load()\n",
    "    for j_, j in enumerate(mask):\n",
    "        for k_, k in enumerate(j):\n",
    "            if k < 21:\n",
    "                pixels[k_,j_] = label_colours[k]\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, ?, 21)\n",
      "(?, ?, ?, 21)\n",
      "(?, ?, ?)\n",
      "(?, ?, ?, 1)\n",
      "(?, ?, ?, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (36, 500, 500, 21) for Tensor 'Placeholder:0', which has shape '(?, 500, 500, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-8daa6b7a5f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mloaded_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_vfeatures_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_img\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_vlabels_array_indexed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloaded_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m#    mIoU, update_op = tf.contrib.metrics.streaming_mean_iou(preds, batch_vlabels_array, num_classes=21)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/systems/tensorflow-env3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/systems/tensorflow-env3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    945\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (36, 500, 500, 21) for Tensor 'Placeholder:0', which has shape '(?, 500, 500, 1)'"
     ]
    }
   ],
   "source": [
    "net = DeepLabLFOVModel()\n",
    "batch_i=0;\n",
    "save_model_path = \"./capstone\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    loaded_graph=tf.get_default_graph()\n",
    "    loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "    loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "    loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    pred = net.preds(loaded_x,loaded_keep_prob)\n",
    "    print(pred.shape)\n",
    "\n",
    "    label_img=tf.placeholder(tf.int8,shape=(None,500,500,1))\n",
    "    mIoU, update_op = tf.contrib.metrics.streaming_mean_iou(pred, label_img, num_classes=21) \n",
    "    \n",
    "    batch_vfeatures, batch_vlabels = load_preprocess_training_batch(batch_i)\n",
    "    \n",
    "    batch_vfeatures_array = np.zeros((len(batch_vfeatures),500,500,3))\n",
    "    batch_vlabels_array = np.zeros((len(batch_vfeatures),500,500,3))\n",
    "    i=0\n",
    "    for b in batch_vfeatures:\n",
    "        batch_vfeatures_array[i] = b\n",
    "        i=i+1\n",
    "    batch_vfeatures_array = batch_vfeatures_array.astype(np.float32)\n",
    "    i=0\n",
    "    for b in batch_vlabels:\n",
    "        batch_vlabels_array[i] = b\n",
    "        i=i+1\n",
    "    batch_vlabels_array_indexed = encode_labels(batch_vlabels_array)\n",
    "          \n",
    "            \n",
    "    preds = sess.run(pred,feed_dict={loaded_x:batch_vfeatures_array,label_img:batch_vlabels_array_indexed,loaded_keep_prob:1.0})\n",
    "#    mIoU, update_op = tf.contrib.metrics.streaming_mean_iou(preds, batch_vlabels_array, num_classes=21) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_vlabels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-8095d3087cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmIoU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming_mean_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_vlabels_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/systems/tensorflow-env3/lib/python3.4/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\u001b[0m in \u001b[0;36mstreaming_mean_iou\u001b[0;34m(predictions, labels, num_classes, weights, metrics_collections, updates_collections, name)\u001b[0m\n\u001b[1;32m   2615\u001b[0m       \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m       \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_collections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_collections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2617\u001b[0;31m       updates_collections=updates_collections, name=name)\n\u001b[0m\u001b[1;32m   2618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/systems/tensorflow-env3/lib/python3.4/site-packages/tensorflow/python/ops/metrics_impl.py\u001b[0m in \u001b[0;36mmean_iou\u001b[0;34m(labels, predictions, num_classes, weights, metrics_collections, updates_collections, name)\u001b[0m\n\u001b[1;32m    755\u001b[0m       name, 'mean_iou', (predictions, labels, weights)):\n\u001b[1;32m    756\u001b[0m     \u001b[0;31m# Check if shape is compatible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;31m# Local variable to accumulate the predictions in the confusion matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "mIoU, update_op = tf.contrib.metrics.streaming_mean_iou(preds, batch_vlabels_array, num_classes=21) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_img= decode_labels(preds[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = Image.fromarray(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAIAAABEtEjdAAAHQElEQVR4nO3Wy20CMABEQYIonNJS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\nSNwBgsQdIEjcAYIuzNAdfG21sP8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x1348D4400>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lblimg=np.array(batch_vlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAIAAABEtEjdAAALNklEQVR4nO3d25HiSAJAUXqiDMEU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\n7izYA8TiqYwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x1348E7438>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(batch_vlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
